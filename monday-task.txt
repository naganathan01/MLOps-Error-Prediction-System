# MLOps Project Enhancement Guide

## 1. Model Lifecycle Management

### MLflow Integration
```python
# Add to your training pipeline
import mlflow
import mlflow.sklearn
import mlflow.xgboost

# Track experiments
with mlflow.start_run():
    mlflow.log_params({"n_estimators": 200, "max_depth": 15})
    mlflow.log_metrics({"auc": 0.93, "precision": 0.89})
    mlflow.sklearn.log_model(model, "model")
    
# Model registry
mlflow.register_model("runs:/<run_id>/model", "ErrorPredictionModel")
```

### Model Versioning Strategy
```yaml
# model_config.yaml
models:
  production:
    version: "v1.2.0"
    model_path: "s3://models/error-prediction/v1.2.0"
    performance_threshold: 0.90
  staging:
    version: "v1.3.0"
    model_path: "s3://models/error-prediction/v1.3.0"
    performance_threshold: 0.92
```

## 2. Data Pipeline Automation

### Apache Airflow DAG
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def retrain_model():
    # Your retraining logic
    pass

def validate_model():
    # Model validation logic
    pass

def deploy_model():
    # Model deployment logic
    pass

dag = DAG(
    'ml_pipeline',
    default_args={
        'owner': 'mlops-team',
        'depends_on_past': False,
        'start_date': datetime(2024, 1, 1),
        'email_on_failure': True,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=5)
    },
    description='ML Model Retraining Pipeline',
    schedule_interval='@daily',
    catchup=False
)

# Define tasks
extract_data = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data_func,
    dag=dag
)

train_model = PythonOperator(
    task_id='train_model',
    python_callable=retrain_model,
    dag=dag
)

validate_model = PythonOperator(
    task_id='validate_model',
    python_callable=validate_model,
    dag=dag
)

deploy_model = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_model,
    dag=dag
)

# Define dependencies
extract_data >> train_model >> validate_model >> deploy_model
```

## 3. Model Monitoring & Drift Detection

### Data Drift Monitoring
```python
import pandas as pd
from scipy import stats

class ModelMonitor:
    def __init__(self, reference_data):
        self.reference_data = reference_data
        
    def detect_drift(self, current_data, threshold=0.05):
        drift_results = {}
        
        for column in self.reference_data.columns:
            if column in current_data.columns:
                # Kolmogorov-Smirnov test
                ks_stat, p_value = stats.ks_2samp(
                    self.reference_data[column], 
                    current_data[column]
                )
                
                drift_results[column] = {
                    'ks_statistic': ks_stat,
                    'p_value': p_value,
                    'drift_detected': p_value < threshold
                }
                
        return drift_results
    
    def performance_monitoring(self, predictions, actuals):
        from sklearn.metrics import accuracy_score, precision_score, recall_score
        
        return {
            'accuracy': accuracy_score(actuals, predictions),
            'precision': precision_score(actuals, predictions),
            'recall': recall_score(actuals, predictions)
        }
```

## 4. Infrastructure as Code

### Terraform Configuration
```hcl
# main.tf
provider "aws" {
  region = var.aws_region
}

# EKS Cluster for model serving
resource "aws_eks_cluster" "ml_cluster" {
  name     = "mlops-cluster"
  role_arn = aws_iam_role.eks_cluster_role.arn
  version  = "1.24"

  vpc_config {
    subnet_ids = var.private_subnet_ids
  }
}

# S3 bucket for model artifacts
resource "aws_s3_bucket" "model_artifacts" {
  bucket = "mlops-model-artifacts-${random_string.suffix.result}"
}

# Lambda function for model inference
resource "aws_lambda_function" "model_inference" {
  filename         = "model_inference.zip"
  function_name    = "ml-model-inference"
  role            = aws_iam_role.lambda_role.arn
  handler         = "handler.lambda_handler"
  runtime         = "python3.9"
  timeout         = 30
  memory_size     = 1024
}
```

## 5. CI/CD Pipeline

### GitHub Actions Workflow
```yaml
# .github/workflows/ml-pipeline.yml
name: ML Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest
    
    - name: Run tests
      run: |
        pytest tests/
    
    - name: Run model training
      run: |
        python scripts/run_pipeline.py
    
    - name: Validate model performance
      run: |
        python scripts/validate_model.py
  
  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Deploy to staging
      run: |
        # Deploy to staging environment
        kubectl apply -f k8s/staging/
    
    - name: Run integration tests
      run: |
        python tests/integration_tests.py
    
    - name: Deploy to production
      if: success()
      run: |
        kubectl apply -f k8s/production/
```

## 6. Observability Stack

### Prometheus Metrics
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# Custom metrics
prediction_counter = Counter('ml_predictions_total', 'Total ML predictions', ['model_version'])
prediction_latency = Histogram('ml_prediction_duration_seconds', 'ML prediction latency')
model_accuracy = Gauge('ml_model_accuracy', 'Current model accuracy')
drift_score = Gauge('ml_data_drift_score', 'Data drift score')

# In your API endpoint
@prediction_latency.time()
def predict(data):
    prediction = model.predict(data)
    prediction_counter.labels(model_version='v1.2.0').inc()
    return prediction
```

### Grafana Dashboard Config
```json
{
  "dashboard": {
    "title": "ML Model Performance",
    "panels": [
      {
        "title": "Prediction Accuracy",
        "type": "stat",
        "targets": [
          {
            "expr": "ml_model_accuracy",
            "legendFormat": "Accuracy"
          }
        ]
      },
      {
        "title": "Prediction Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, ml_prediction_duration_seconds_bucket)",
            "legendFormat": "95th percentile"
          }
        ]
      }
    ]
  }
}
```

## 7. Security & Compliance

### Model Security
```python
# Add authentication and authorization
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    if credentials.credentials != "your-secret-token":
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials"
        )
    return credentials

@app.post("/predict")
async def predict(data: PredictionRequest, credentials = Depends(verify_token)):
    # Your prediction logic
    pass
```

## 8. Feature Store Integration

### Feature Store Architecture
```python
# Feature store integration
from feast import FeatureStore

fs = FeatureStore(repo_path=".")

# Define feature service
from feast import FeatureService

error_prediction_fs = FeatureService(
    name="error_prediction_features",
    features=[
        system_metrics_fv[["cpu_usage", "memory_usage", "disk_usage"]],
        network_metrics_fv[["latency", "throughput"]],
        application_metrics_fv[["error_rate", "response_time"]]
    ]
)

# Get features for prediction
features = fs.get_online_features(
    features=[
        "system_metrics:cpu_usage",
        "system_metrics:memory_usage",
        "network_metrics:latency"
    ],
    entity_rows=[{"server_id": "server_001"}]
).to_dict()
```

## Interview Talking Points

### Technical Depth
- "I implemented a complete MLOps pipeline using MLflow for experiment tracking and model registry"
- "Built automated retraining pipelines with Airflow that trigger based on data drift detection"
- "Deployed models on Kubernetes with horizontal pod autoscaling and blue-green deployments"

### Business Impact
- "Reduced model deployment time from weeks to minutes through automated CI/CD"
- "Improved model reliability with 99.9% uptime through proper monitoring and alerting"
- "Decreased manual intervention by 80% through automated drift detection and retraining"

### Problem-Solving Skills
- "Handled model versioning and rollback strategies for production incidents"
- "Implemented A/B testing framework to validate model improvements safely"
- "Built comprehensive monitoring to detect both data drift and model performance degradation"
